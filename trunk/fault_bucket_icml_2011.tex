\documentclass{article}

\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[draft]{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Appearing in''
% \usepackage[accepted]{icml2011}

\usepackage{icml2011}

\usepackage{xcolor}
\usepackage[american]{babel}
\usepackage{auto-pst-pdf}
\usepackage{microtype}
\usepackage{psfrag}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage[T1]{fontenc}
\usepackage{nicefrac}
\usepackage{booktabs}

\usepackage{natbib}

\newcommand{\psff}[1]{\input{#1.tex}\includegraphics{#1.eps}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\deq}{\ensuremath{\triangleq}}
\newcommand{\given}{\ensuremath{\mid}}
\newcommand{\cm}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\bm}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\data}{\ensuremath{\cm{D}}}
\newcommand{\inv}{\ensuremath{^{-1}}}
\newcommand{\acro}[1]{\textsc{#1}}

% Mike's definitions
\newcommand{\dd}[2]{\delta(#1-#2))}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\vz}{\vect{z}}
\newcommand{\vx}{\vect{x}}
\newcommand{\vf}{\vect{f}}
\newcommand{\vs}{\vect{\sigma}}
\newcommand{\amean}[2]{\tilde{{m}}(#1 \given #2 )}
\newcommand{\acov}[2]{\tilde{{C}}(#1 \given #2 )}
\newcommand{\p}[2]{p(#1 \given #2 )}
\newcommand{\fPr}{p}
\newcommand{\Prob}[2]{\fPr(#1 \given #2 )}
\newcommand{\ps}[2]{p(#1\vert#2)}
\newcommand{\mean}[2]{{m}(#1 \given #2 )}
\newcommand{\cov}[2]{{C}(#1 \given #2 )}
\newcommand{\N}[3]{\cm{N}( #1;#2,#3 )}
\newcommand{\st}{_{\star}}
\newcommand{\tr}{\ensuremath{\mathsf{T}}}
\newcommand{\defequal}{\triangleq}

\DeclareMathOperator{\fault}{fault}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\chol}{chol}

\icmltitlerunning{Fault Bucket for Time-Series Prediction}

\nonstopmode

\begin{document}

\twocolumn[ 

  \icmltitle{A ``Fault Bucket'' for Time-Series Prediction with
    Uncharacterized Faulty Observations}

  \icmlkeywords{Gaussian processes, time-series prediction, fault
    detection, hyperparameter marginalization}

\vskip 0.3in
]

\begin{abstract}
  We provide a proposal for performing both online prediction and
  retrospective inference of signals from observations that are
  potentially rendered less informative than normal due to a faulty
  observation mechanism.  The proposed model uses Gaussian processes
  and a general ``fault bucket'' for \textit{a priori} uncharacterized
  faults, along with an approximate method for marginalizing the
  potential faultiness of all observations. This gives rise to an
  efficient, flexible algorithm. We demonstrate our method's relevance
  to several problems drawn from environmental-monitoring
  applications.
\end{abstract}

\section{Introduction}

We consider the problem of inferring a signal $y\colon \R \to \R$ from
noisy measurements of it.  Typical algorithms for this purpose often
assume that the data is linear, i.i.d., Markovian, or Gaussian. In
some cases, knowledge about the problem can enable the explicit
specification of parameterized nonlinear models. This approach,
however, is problematic for two reasons. First, estimating the model
parameters is often difficult. More crucially, collected observations
in real-world applications are often corrupted in non-trivial ways due
to, for example, a faulty sensing mechanism. Such effects often
inhibit effective environmental monitoring, where data can be
corrupted in unknown ways, rendering the \emph{a priori} construction
of parametric model impossible. The motivating example for this paper
is the related fast-growing field of water-quality monitoring
\citep{wagner2006guidelines}. Despite the enormous importance of
water-quality monitoring to humans, there is a lack of sound
statistical machine-learning solutions for this problem. With this
study, we hope to release some of this data to the public domain
(following the reviewing process in order to satisfy anonymous
reviewing) and to present new machine-learning techniques to meet the
demands of field.

Our proposed method for predicting signals with faulty observation
mechanisms will rely on Gaussian processes (\acro{gp}s) to perform
inference about the underlying latent function.  Previous work has
approached this problem by creating observation models that specify
the anticipated potential fault types \textit{a priori}
\citep{garnettosborne}, but this might be an unreasonable assumption
in highly variable or poorly understood environments.  Here we suggest
the use of a catch-all ``fault bucket,'' which can identify and treat
appropriately data readings suspected of being corrupted in some way.
The result is a method for data-stream prediction that can manage a
wide range of faulty observations without requiring significant
domain-specific knowledge.

The collection of literature on similar topics is vast, under labels
such as fault detection \citep{deFreitas1996, Eciolaza2001,
  Isermann2005, Ding2008}, novelty detection \citep{Markou2003},
anomaly detection \citep{Chandola:2009}, or one-class classification
\citep{Khan2010}. Despite this, most of the techniques are too simple
(e.g. linear or Gaussian) or fail to produce good uncertainty
estimates. Additionally, many problems in anomaly detection and
one-class classification are of a very different nature and,
therefore, the techniques developed there are not immediately
applicable to our domain of interest. Uncertainty estimates are key in
order to provide the user of a system with reliable monitoring
signals. Green-tech areas, including environmental monitoring and
energy-demand prediction, are still far from full
automation. Currently, the most important requirement of such systems
is to provide the user with signals that he or she can use to reach a
decision, making the uncertainty of predictions of the utmost
importance. For this reason, we focus on developing \acro{gp}-based
techniques to build probabilistic nonlinear models of the latent and
fault processes and thereby deliver reliable reports.  In addition to
providing posterior probabilities of observation faultiness, we are
able to perform effective prediction for the latent process even in
the presence of faults.

GPs have been used previously for fault detection
\citep{Eciolaza2001}. However, the nature of the data and setup in
that domain is different from ours, forcing us to develop new
\acro{gp}-based techniques for detection and prediction in the
presence of faults.

\section{Gaussian Processes}

Gaussian processes provide a simple, flexible framework for performing
Bayesian inference about functions \citep{gpml}.  A Gaussian process
is a distribution on the functions $y\colon \cm{X} \to \R$ (on an
arbitrary domain $\cm{X}$) with the property that the distribution of
the function values at a finite subset of points $F \subseteq \cm{X}$
are multivariate Gaussian distributed.

A Gaussian process is completely defined by its first two moments: a
mean function $\mu\colon \cm{X} \to \R$ and a symmetric positive
semidefinite covariance function $K\colon \cm{X} \times \cm{X} \to
\R$.  The mean function describes the overall trend of the function
and is typically set to a constant for convenience.  The covariance
function describes how function values are correlated as a function of
their locations in the domain, thereby encapsulating information about
the overall shape and behavior of the signal.  Many covariance
functions are available to model a wide variety of anticipated
signals.

Suppose we have chosen a Gaussian process prior distribution on the
function $y\colon \cm{X} \to \R$, and a set of input points $\bm{x}$,
the prior distribution on $\bm{y} \deq y(\bm{x})$ is
\begin{equation*}
 p(\bm{y} \given \bm{x}, \theta)
 =
 \cm{N}
 \bigl(
   \bm{y};
   \mu(\bm{x}; \theta),
   K(\bm{x}, \bm{x}; \theta)
 \bigr),
\end{equation*}
where $K(\bm{x}, \bm{x}; \theta)$ is the Gram matrix of the points
$\bm{x}$, and $\theta$ is a vector containing any parameters required
of $\mu$ and $K$, which constitute hyperparameters of the model.

Exact measurements of the latent function are typically not available;
however, we may combine the Gaussian process distribution on the
latent function with an observation model that takes into account
potential noise in our measurements.  Let $z(x)$ represent the
realization of an observation of the signal at $x$ and $y(x)$
represent the value of the unknown true latent signal at that point.
When the observation mechanism is not expected to experience faults,
the usual noise model used is
\begin{equation}\label{iidnoise}
 p(z \given y, x, \sigma_n^2)
 \deq
 \cm{N}(z; y, \sigma_n^2),
\end{equation}
which represents additive i.i.d.\space Gaussian observation noise with
variance $\sigma_n^2$. Note that this model is inappropriate when
sensors can experience faults, which complicate the relationship
between $z$ and $y$.

With the observation model above, given a set of observations
$
 \data
 \deq
 \bigl\lbrace
   \bigl( x, z(x) \bigr)
 \bigr\rbrace
 \deq
 ( \bm{x}, \bm{z} )
$,
the posterior distribution of $y\st \deq y(x\st)$ given these data is
\begin{equation*}
 p(y\st \given \data, \theta)
 =
 \cm{N}
 \bigl(
   y\st;
   \mean{y\st}{\data,\theta},
   \cov{y\st}{\data,\theta}
 \bigr),
\end{equation*}
where the posterior mean and covariance are
\begin{align*}
 &
 \mean{y\st}{\data,\theta}
 \deq
 \mu(x\st; \theta)
 +
 {}
 \\
 &
 \mspace{20mu}
 +
 K(x\st, \bm{x}; \theta)
 \bigl(
 K(\bm{x}, \bm{x}; \theta) + \sigma_n^2 \bm{I}
 \bigr)\inv
 \bigl(
   \bm{z} - \mu(\bm{x}; \theta)
 \bigr)
 \\  
 &
 \cov{y\st}{\data,\theta}
 \deq
 K(x\st, x\st; \theta)
 -
 {}
 \\
 &
 \mspace{20mu}
 -
 K(x\st, \bm{x}; \theta)
 \bigl(
   K(\bm{x}, \bm{x}; \theta) + \sigma_n^2 \bm{I}
 \bigr)\inv
 K(\bm{x}, x\st; \theta).
\end{align*}
Notice that the posterior is also a Gaussian process. 

We now make some definitions for the sake of readability. Henceforth,
we assume that our observations $\vz$ have already been scaled by the
subtraction of the prior mean $\mu(\bm{x}; \theta)$. We will also make
use of the covariance matrix shorthand $K_{m,n} \defequal
K(\vx_m,\vx_n)$. Finally, for now, we'll drop the explicit dependence
of our probabilities on the hyperparameters $\theta$ (it will be
implicitly assumed that all quantities are conditioned on knowledge of
them), and will return to them later.

\section{Fault Bucket}\label{bucket}

Rather than specifying explicit parameters for every possible fault
type, we propose a single catch-all ``fault bucket'' that can identify
and treat appropriately measurements that are suspected of being
faulty.  The basic idea is to model faulty observations as being
generated from a Gaussian distribution with a very wide variance;
points that are more likely under this model than under the normal
predictive model of the Gaussian process can reasonably be assumed to
be corrupted in some way, assuming we have a good understanding of the
latent process. It is hoped that a very broad class of faults can be
captured in this way.

To formalize this idea, we choose an observation noise distribution to
replace that in \eqref{iidnoise} that models the noise as independent
but not identically distributed with separate variances for the
non-fault and fault cases.
\begin{align*}
 p(z \given y, x, \neg\fault, \sigma_n^2)
 &
 \deq
 \cm{N}(z; y, \sigma_n^2)
 \\
 p(z \given y, x, \fault, \sigma_f^2)
 &
 \deq
 \cm{N}(z; y, \sigma_f^2),
\end{align*}
where $\fault \in \lbrace 0, 1 \rbrace$ is a binary indicator of
whether the observation $z(x)$ was faulty and $\sigma_f > \sigma_n$ is
the standard deviation around the mean of faulty measurements.  The
values of both $\sigma_n$ and $\sigma_f$ form hyperparameters of our
model and are therefore included in $\theta$.

Of course, {\it a priori}, we do not know whether any given
observation will be faulty.  Unfortunately, managing our uncertainty
about the ``faultiness'' of all available observations is a
challenging task. If we have $N$ observations available, then there
are $2^N$ possible assignments of faultiness. For even moderately
sized datasets, it is computationally infeasible to correctly
marginalize over all these possible values.

Instead, we propose a sequential approach, applicable for ordered data
such as time series. For time series, the value to be predicted $y\st$
typically lies in the future, and less-recent observations are
typically less pertinent for this task than more-recent ones. The
intuition behind our approach is to, at a given time step, ``merge''
our current model with the uncertainty present in the inferred
faultiness of the most recent observation. In effect, we represent
each observation as having a known variance lying between between
$\sigma_n^2$ and $\sigma_f^2$. The more likely an observation's
faultiness, the closer its assigned variance will be to the (large)
fault variance and the less relevant it will become for inference
about the latent process. This approximate observation is then used
for future predictions; we need never consider the full sum over all
observations. Nonetheless, this approximate marginalization over
faultiness is preferable to heuristics that would designate all
observations as either faulty or not; our method acknowledges the
uncertainty that may exist in our belief about faultiness.

More formally, imagine that we have partitioned our observations
$\data_{a,b}$ into a set of old observations
$\data_a\deq(\vx_a,\vz_a)$ and a set of newer observations $\data_b
\deq (\vx_b,\vz_b)$. Define $\vs_{a}$ to be the (unknown) vector of
all noise variances at observations $\vz_{a}$, and define $\vs_{b}$
similarly. Because we have to sum over all possible values for these
vectors, we will index the $2^{\lvert a \rvert}$ possible values of
$\vs_{a}$ by $i$ (each given by a different combination of
faultinesses over $\data_a$) and the values of $\vs_{b}$ similarly by
$j$. We now define the covariances
\begin{align*}
 V_a^i & \defequal K_{a,a} + \diag \vs_{a}^i, \\
 V_b^j & \defequal K_{a,a} + \diag \vs_{a}^i, \\
 V_{a,b}^{i,j} & \defequal K_{\{a,b\},\{a,b\}} + \diag \{\vs_{a}^i,\vs_{b}^j\},
\end{align*}
where $\diag \vs$ is the diagonal matrix with diagonal $\vs$. 

To initialize our algorithm, imagine that $a$ identifies a small set
of data, such that we can readily compute
\begin{equation}
 \hspace{-0.0cm}p(\vz_a)\!=\!\sum_i  \ps{\vz_a}{\vs^i_a}\fPr(\vs^i_a)
\!=\! \sum_i \N{\vz_a}{0}{V_a^i}\fPr(\vs_a^i) \label{eq:likelihood_a}
\end{equation}
(which is the likelihood of our hyperparameters), so
\begin{equation*}
\ps{\vs_a}{\data_{a}} 
= \frac{\ps{\vz_a}{\vs_a}\fPr(\vs_a)}{p(\vz_a)} 
= \frac{\N{\vz_a}{0}{V_a} \fPr(\vs_a)}{p(\vz_a)}\label{eq:psa}.
\end{equation*}
This distribution also specifies the probability of our observations
$\data_a$ being faulty; for a single observation $\data_a$,
$
p\bigl(\fault(\data_a) \given \data_{a}\bigr) = \Prob{\sigma_a = \sigma_f}{\data_{a}}
$.

If we were to perform predictions for some $y\st$ using $\data_a$
alone, we would need to evaluate
\begin{align*}
&\p{y\st}{\data_{a}} = \sum_{i} \Prob{\vs^i_{a}}{\data_a} \p{y\st}{\data_a, \vs^{i}_{a}}\nonumber\\
&=\sum_{i} \Prob{\vs^i_{a}}{\data_a} \cm{N}\bigl(y\st; \mean{y\st}{\data_a, \vs^{i}_{a}}, \cov{y\st}{\data_a, \vs^{i}_{a}}\bigr),
\end{align*}
the weighted sum of Gaussian predictions made using the different
possible values for $\vs_{a}$.  Now, we will perform the ``merging''
that we discussed earlier, by approximating this sum of Gaussians as a
moment-matched single Gaussian. It is our hope that our predictions
for $y\st$ are not so sensitive to the noise in our observations that
all the Gaussians in this sum become dramatically different. In any
case, the quality of this approximation will improve over time---if
$y\st$ is far removed from our old data $\data_a$, then our
predictions really will not be very sensitive to $\sigma_a$. We now
approximate
\begin{align*}
 &\p{y\st}{\data_{a}} \simeq \cm{N}\bigl(y\st; \amean{y\st}{\data_a}, \acov{y\st}{\data_a}\bigr),\label{eq:pya}
\end{align*}
where we have
\begin{align}
\amean{y\st}{\data_{a}} \defequal {}& K_{\star,a} \tilde{V}_a^{-1} \vz_a,\label{eq:ameana}\\
\acov{y\st}{\data_{a}}
\defequal {}& K_{\star,\star} - K_{\star,a}(\tilde{V}_a^{-1}-\tilde{W}_a^{-1})K_{a,\star} - \nonumber\\
& - \amean{y\st}{\data_{a,b}}^2 ,\label{eq:acova}
\end{align}
where
\begin{align}
 \tilde{V}_a^{-1}  & \defequal \sum_i \Prob{\vs^i_{a}}{\data_a} (V_a^i)^{-1},\nonumber\\
 \tilde{W}_a^{-1} & \defequal \sum_i \Prob{\vs^i_{a}}{\data_a} (V_a^i)^{-1}\vz_a \vz_a^\tr (V_a^i)^{-1}.\label{eq:Wa}
\end{align}
Note that for $\tilde{W}_a$, explicitly computing (unstable) matrix
inverses can be avoided by solving the appropriate linear equations
using Cholesky factors.  For $\tilde{V}_a$, we can rewrite
$(A^{-1}+B^{-1})^{-1} = A (A+B)^{-1} B$. If $i\in\{0,1\}$ (as it would
be if $a$ identified a single observation which could be either faulty
or not),
\begin{align} \label{eq:inverse_trick}
\tilde{V}_a & = V^0_a\bigl(
\Prob{\vs^1_{a}}{\data_a} V^0_a 
+ 
\Prob{\vs^0_{a}}{\data_a} V^1_a
\bigr)^{-1}V^1_a.
\end{align}
If $i$ takes more than two values, we can simply iterate using the
same technique. Having used \eqref{eq:inverse_trick} to compute
$\tilde{V}_a$, we can then calculate
\begin{align*}
 \tilde{R}_a & \defequal \chol \tilde{V}_a \label{eq:Ra}, \\
 \tilde{T}_a & \defequal \chol (\tilde{R}_a)^{-1} \vz_a \label{eq:Ta},
\end{align*}
and use them, along with \eqref{eq:Wa}, to efficiently determine
\eqref{eq:ameana} and \eqref{eq:acova}.

Now, with these calculations performed, we can imagine receiving
further data $\data_b$. Predictions are now performed as
\begin{align*}
\p{y\st}{\data_{a,b}} & = \sum_{i} \sum_{j} \p{y\st}{\data_{a,b}, \vs^{i,j}_{a,b}} \Prob{\vs^{i,j}_{a,b}}{\data_{a,b}}.
\end{align*}
The full sums here can easily become too large to actually
evaluate. To simplify, we assume that our later observations are
independent of the noise in our earlier observations. To be more
precise, we make the approximations
\begin{align} \label{eq:approx}
\Prob{\vs^{i,j}_{a,b}}{\data_{a,b}} & \simeq \Prob{\vs^i_{a}}{\data_a}\,\Prob{\vs^j_{b}}{\data_{a,b}}
\end{align}
and
\begin{align}
p(y\st \given& \data_{a,b}) &\nonumber\\
\simeq {}& \sum_{j} \Prob{\vs^j_{b}}{\data_{a,b}}\sum_{i} \Prob{\vs^i_{a}}{\data_a} \p{y\st}{\data_{a,b}, \vs^{i,j}_{a,b}}\nonumber\\
= {}&\sum_{j} \Prob{\vs^j_{b}}{\data_{a,b}}\sum_{i} \Prob{\vs^i_{a}}{\data_a}\nonumber \times {}\\
&\times \cm{N}\bigl(y\st; \mean{y\st}{\data_{a,b}, \vs^{i,j}_{a,b}}, \cov{y\st}{\data_{a,b}, \vs^{i,j}_{a,b}}\bigr).\label{eq:sum_o_Gaussians}
\end{align}
Before trying to manage these sums, we will determine $p(\vs_b \given
\data_{a,b})$. As before, this distribution gives us the probability
of the observations $\data_b$ being faulty. For example, if we have
only a single observation $\data_b$, $
p\bigl(\fault(\data_b) \given \data_{a,b}\bigr) = \Prob{\sigma_b =
  \sigma_f}{\data_{a,b}} $.

Moving on, we define
\begin{align*}
\amean{\vz_b}{\data_a} \defequal {} &
K_{b,a} \tilde{V}_a^{-1} \vz_a \label{eq;ameanba}
\\
\acov{\vz_b}{\data_{a},\vs_b}
\defequal {} & V_b - K_{b,a}(\tilde{V}_a^{-1}-\tilde{W}_a^{-1})K_{a,b} - {} \nonumber\\
& - \amean{\vz_b}{\data_{a,b}}^2, \label{eq;acovba}
\end{align}
where both $\tilde{V}_a$ (or its Cholesky factor) and
$\tilde{W}_a^{-1}$ were computed previously. By using
\eqref{eq:approx} and again approximating a sum of Gaussians as a
single Gaussian,
\begin{align*}
\Prob{\vs_b}{\data_{a,b}} & = \frac{\sum_i \p{\vz_b}{\data_a,\vs^i_{a,b}}p(\vz_a,\vs^i_{a,b})}{p(\vz_{a,b})}\nonumber\\
& \simeq \frac{\sum_i \p{\vz_b}{\data_a,\vs^i_{a,b}}\fPr(\vs_a^i\mid{\data_a})\fPr(\vs_{b})}{\ps{\vz_{b}}{\data_a}}\nonumber\\
& \simeq \frac{\cm{N}\bigl(\vz_b; \amean{\vz_b}{\data_a}, \acov{\vz_b}{\data_a, \Sigma_b}\bigr) \fPr(\vs_b)}{\p{\vz_{b}}{\data_a}},\label{eq:psb}
\end{align*}
where we have
\begin{align}
p(\vz_{b} &\given \data_a)\nonumber\\
& = \sum_i \sum_j \p{\vz_b}{\data_a,\vs^i_{a,b}}\Prob{\vs^{i,j}_{a,b}}{\data_a}\nonumber\\
& \simeq \sum_i \sum_j \p{\vz_b}{\data_a,\vs^i_{a,b}}\Prob{\vs_a^i}{\data_a}\fPr(\vs_{b}^j)\nonumber\\
& \simeq \sum_j \cm{N}\bigl(\vz_b; \amean{\vz_b}{\data_a}, \acov{\vz_b}{\data_a, \vs_b^j}\bigr) \fPr(\vs_b^j).\label{eq:likelihood_b}
\end{align}
Note that the product of \eqref{eq:likelihood_b} and
\eqref{eq:likelihood_a} gives the likelihood of our hyperparameters.

Now, returning to \eqref{eq:sum_o_Gaussians}, we will once again
approximate a sum of Gaussians as a moment-matched single
Gaussian. Our goal here is to reuse our previously evaluated sums over
$i$ to resolve future sums over $i$. As we gain more data, the
faultiness of very old data becomes less important. We arrive at
\begin{align}
\p{y\st}{\data_{a,b}} & \simeq \N{y\st}{\amean{y\st}{\data_{a,b}}}{\acov{y\st}{\data_{a,b}}}\,,\label{eq:pyab}
\end{align}
where we have
\begin{align*}
\amean{y\st}{\data_{a,b}} \defequal {}&  K_{\star,a,b} \tilde{V}_{a,b}^{-1} \vz_{a,b}\label{eq:ameanab}\\
\acov{y\st}{\data_{a}}
\defequal {} & K_{\star,\star} - K_{\star,a}(\tilde{V}_{a,b}^{-1}-\tilde{W}_{a,b}^{-1})K_{a,\star} - {} \nonumber\\
& - \amean{y\st}{\data_{a,b}}^2 \,.\label{eq:acovab}
\end{align*}
where
\begin{align*}
\tilde{V}^{-1}_{a,b} \defequal {} &
\sum_{j} \Prob{\vs^j_{b}}{\data_{a,b}}\sum_i \Prob{\vs^i_{a}}{\data_{a}} (V_{a,b}^{i,j})^{-1} \\
\tilde{W}^{-1}_{a,b} \defequal {} &
\sum_{j} \Prob{\vs^j_{b}}{\data_{a,b}}\sum_i \Prob{\vs^i_{a}}{\data_{a}} (V_{a,b}^{i,j})^{-1}  \times {} \\
& \times \vz_{a,b}^{\phantom{\tr}} \vz_{a,b}^\tr (V_{a,b}^{i,j})^{-1}.
\end{align*}
Now, using the inversion by partitioning formula \citep[Section 2.7]
{NumericalRecipes},
\begin{align*}
\hspace{-0.2cm}&({V}^{i,j}_{a,b})^{-1} = \\
&\begin{bmatrix}
 S^{i,j}_a &\hspace{-0.1cm} -S^{i,j}_a K_{a,b} (V^j_b)^{-1} \\
 - (V^j_b)^{-1} K_{b,a} S^{i,j}_a &\hspace{-0.1cm} (V^j_b)^{-1}\!+\!(V^j_b)^{-1} K_{b,a} S^{i,j}_a K_{a,b} (V^j_b)^{-1} 
\end{bmatrix},
\end{align*}
where
$
S^{i,j}_a \defequal (V^i_a -K_{a,b} (V^j_b)^{-1}K_{b,a})^{-1}\,.
$

Note that $({V}^{i,j}_{a,b})^{-1}$ is affine in $S^{i,j}_a$, so that when
\begin{equation}\label{eq:V_a_big}
 V_a \gg K_{a,b} V_b^{-1} K_{b,a}\,,
\end{equation}
$({V}^{i,j}_{a,b})^{-1}$ is effectively affine in $(V^i_a)^{-1}$. This
is true if given $\data_b$, it is impossible to accurately predict
$\data_a$. This might be the case if $\data_a$ represents a lot of
information relative to $\data_b$ (if, for example, $\data_a$ is our
entire history of observations where $\data_b$ is simply the most
recent observation), or if $\data_b$ and $\data_a$ are simply not
particularly well correlated. On this basis, \eqref{eq:V_a_big} seems
reasonable for our application. Defining the affine map $f\colon (V^i_a)^{-1}
\mapsto ({V}^{i,j}_{a,b})^{-1}$, then, noticing $\sum_i \Prob{\vs^i_{a}}{\data_{a}} = 1$,
$$
\sum_i \Prob{\vs^i_{a}}{\data_{a}} f\bigl((V^i_a)^{-1} \bigr) \simeq f\Bigl(\sum_i \Prob{\vs^i_{a}}{\data_{a}}(V^i_a)^{-1} \Bigr).
$$
Therefore, for
\begin{align*}
\hat{V}^{j}_{a,b} & \defequal
\begin{bmatrix}
 \tilde{V}_a & K_{a,b}\\
 K_{b,a} & V^j_b
\end{bmatrix},
\\
\tilde{V}^{-1}_{a,b} &\simeq \sum_j \Prob{\vs^j_{b}}{\data_{a,b}} (\hat{V}^{j}_{a,b})^{-1},
\nonumber\\
\tilde{V}_{a,b} & \simeq
\begin{bmatrix}
 \tilde{V}_a & K_{a,b}\\
 K_{b,a} & \tilde{V}_{b|a} + K_{b,a} \tilde{V}_a^{-1} K_{a,b}
\end{bmatrix},
\end{align*}
where
\begin{equation*}
 \tilde{V}^{-1}_{b|a} \defequal \sum_j \Prob{\vs^j_{b}}{\data_{a,b}} (V^j_b -K_{b,a} \tilde{V}_a^{-1}K_{a,b})^{-1}
\end{equation*}
can be computed using the same trick as in \eqref{eq:inverse_trick} if
$b$ identifies a single observation and $j\in\{0,1\}$. Note that the
lower right hand element of $\tilde{V}_{a,b}$ defines the noise
variance to be associated with observations $\data_b$. As before, we
determine our predictions \eqref{eq:pyab} by solving linear equations
using the quantities
\begin{align}
 \tilde{R}_{a,b} & \defequal \chol \tilde{V}_{a,b} \label{eq:Rab}, \\
 \tilde{T}_{a,b} & \defequal \chol (\tilde{R}_{a,b})^{-1} \vz_a \label{eq:Tab};
\end{align}
both of which can be efficiently determined \citep[Appendix
  B]{osbornebayesian} using the evaluated $\tilde{R}_a$ and
$\tilde{T}_a$.

We now turn to $\tilde{W}_{a,b}^{-1}$. Unfortunately, even if
\eqref{eq:V_a_big} were true, $\tilde{W}_{a,b}^{-1}$ is quadratic in
$(V^i_a)^{-1}$. We will nonetheless assume that $\tilde{W}_{a,b}^{-1}$
is affine in $(V^i_a)^{-1}$. The quality of our approximation for
$\tilde{W}_{a,b}^{-1}$ is much less critical than for
$\tilde{V}^{-1}_{a,b}$, because the former only influences the
variance of our predictions for the current predictant; any flaws in
that approximation will not be propagated forward. Further, of course,
if one probability dominates, 
\begin{equation*}
\Prob{\vs^i_{a}}{\data_{a}}\gg
\Prob{\vs^{i'}_{a}}{\data_{a}} \quad \forall i' \neq i,
\end{equation*} 
then the approximation is valid. With our approximation,
\begin{align}
\tilde{W}^{-1}_{a,b} \defequal
& \sum_{j} \Prob{\vs^j_{b}}{\data_{a,b}} (\hat{V}_{a,b}^{j})^{-1}\vz_{a,b}^{\phantom{\tr}} \vz_{a,b}^\tr (\hat{V}_{a,b}^{i,j})^{-1}\,.\label{eq:Wab}
\end{align}
and we can solve for
$K_{\star,(a,b)}\tilde{W}^{-1}_{a,b}K_{(a,b),\star}$ by efficiently
updating using the previously computed quantity $\tilde{T}_{a}$.

%% \begin{algorithm}[tb]
%%    \caption{Fault Bucket}
%% \label{alg:fault_bucket}
%% \begin{algorithmic}
%%    \STATE {\bfseries input:} data $(\vx,\vz)$, lookahead $\delta$
%% \STATE $x_a \leftarrow x_1,\, z_a \leftarrow z_1,
%% \,x\st \leftarrow x_{1+\delta}$
%% \\
%% \STATE {\bfseries return:} $p(\vz_a) \leftarrow$\eqref{eq:likelihood_a}
%% \FOR {$i = 0,1$}
%% \STATE {\bfseries return:} $\Prob{\vs^i_a}{\data_{a}} \leftarrow$ \eqref{eq:psa}\\
%% \ENDFOR
%%  \STATE $\tilde{R}_a \leftarrow$ \eqref{eq:Ra}, $\tilde{T}_a\leftarrow$ \eqref{eq:Ta}, \, $\tilde{W}^{-1}_a \leftarrow$ \eqref{eq:Wa}\\
%%  \STATE $\amean{y\st}{\data_a} \leftarrow$ \eqref{eq:ameana}, $\acov{y\st}{\data_a} \leftarrow$ \eqref{eq:acova}\\
%% \STATE {\bfseries return:} $\p{y\st}{\data_{a}} \leftarrow$ \eqref{eq:pya}\\
%% \FOR {$t = 2,\ldots$}
%% \STATE $x_b \leftarrow x_t, z_b \leftarrow z_t, x\st \leftarrow x_{t+\delta}$\\
%% \STATE $\amean{\vz_b}{\data_a} \leftarrow$ \eqref{eq;ameanba}, $\acov{\vz_b}{\data_a} \leftarrow$ \eqref{eq;acovba}\\
%% \STATE $\p{\vz_b}{\data_a} \leftarrow$ \eqref{eq:likelihood_b}\\
%% \FOR {$j = 0,1$}
%% \STATE {\bfseries return:} $\Prob{\vs^j_b}{\data_{a,b}} \leftarrow$ \eqref{eq:psb}\\
%% \ENDFOR
%% \STATE $\tilde{R}_{a,b} \leftarrow$ \eqref{eq:Rab}, $\tilde{T}_{a,b}\leftarrow$ \eqref{eq:Tab}, \, $\tilde{W}^{-1}_{a,b} \leftarrow$ \eqref{eq:Wab}\\
%%  \STATE $\amean{y\st}{\data_{a,b}} \leftarrow$ \eqref{eq:ameanab}, $\acov{y\st}{\data_{a,b}} \leftarrow$ \eqref{eq:acovab}\\
%% \STATE {\bfseries return:} $\p{y\st}{\data_{a,b}}\leftarrow$ \eqref{eq:pyab}\\
%% \STATE {\bfseries return:} $p(\vz_a) \leftarrow p(\vz_a)\p{\vz_b}{\data_a}$
%% \STATE $x_a \leftarrow \{x_a,x_b\},\, z_a \leftarrow \{z_a,z_b\}$\\
%% \STATE $\tilde{R}_a \leftarrow \tilde{R}_{a,b},\, \tilde{T}_a\leftarrow \tilde{T}_{a,b}$\\
%% \ENDFOR 
%% \end{algorithmic}
%% \end{algorithm}

%% An outline of our approach is depicted in Algorithm \ref{alg:fault_bucket}.

\subsection{Discussion}

We return to the management of our hyperparameters
$\theta$. Unfortunately, analytically marginalizing $\theta$ is
impossible. Most of the hyperparameters of our model can be set by
optimizing their likelihood on a large training set, giving a
likelihood close to a delta function. This is not true of the
hyperparameters $\sigma_n$ and $\sigma_f$, due to exactly the same
problematic sums discussed earlier. Instead, we marginalize these
hyperparameters online using Bayesian Monte Carlo \citep[Chapter
  7]{osbornebayesian}, taking a fixed set of samples in their values
and using the hyperparameter likelihoods (computed in
\eqref{eq:likelihood_a} and \eqref{eq:likelihood_b}) to construct
weights over them. Essentially, we proceed as described above
independently in parallel for each sample, and combine the predictions
from each in a final weighted mixture for prediction. Note that we can
use a similar procedure \citep{garnettosborne} to determine the full
posterior distributions of $\sigma_n$ and $\sigma_f$, if desired.  It
would be desirable to use non-fixed samples, but, unfortunately, this
would require reconstructing our observation covariance matrix from
scratch each time a sample is moved.

The proposal above can be extended in several ways. First, we may wish
to sum over more than one fault variance, which could be useful if
observations are prone to faultiness in more than one mode.  Note that
if, instead of summing over a small number of known variances, we
wished to marginalize with respect to a density over noise variance,
we can simply replace the sums over $i$ and $j$ with appropriate
integrals. Obvously this will only be analytically possible if the
posteriors for $\sigma_a$ take appropriate, simple forms.  These
extensions would allow our algorithm to tackle the general problem of
heteroskedasticity.

Our proposed algorithm steps through our data one at a time, so that
$\data_b$ always contains only a single observation. However, it would
be possible in general to step in larger chunks, evaluating larger
sums. Although more computationally demanding, this might be expected
to improve results. It would also allow us to consider non-diagonal
noise contributions.

We have so far not specified our prior for faultiness (as expressed by
$p(\sigma_a)$ and $p(\sigma_b)$). Within this paper, we consider
exclusively a time-independent probability of faultiness. However,
within the framework afforded by our approximation \eqref{eq:approx},
we are free to consider noise variances that are expected to change
over time.

In some contexts it might be useful to perform inference about the
fault contribution, rather than the signal of interest.  This task is
trivial; we merely switch the roles of the fault and non-fault
contributions.  To make a prediction about a potential faulty signal
at $x$, we follow exactly the same procedure as above, substituting
$\sigma_n^2$ for $\sigma_f^2$.

\begin{figure*}
  \centering
  \psff{bias}
  \psff{kf_bias}
  \psff{dynamics}
  \psff{kf_dynamics}
  \label{compare}
\end{figure*}

\begin{figure*}
  \centering
  \psff{painting}
  \psff{fishkiller}
  \label{justfb}
\end{figure*}

Finally, some applications might required a hard decision about
whether a particular observation was faulty.  This would be necessary,
for example, if a system had correctional or responsive actions that
it could take when such an event occurred.  Fortunately, we can
address this problem using simple Bayesian decision theory.

\section{Results}
We test the effectiveness of the fault bucket on several time-series
datasets that are indicative of problems found in environmental
monitoring. In particular, we test on water-level readings; such data
are often characterized by complex dynamics and will therefore provide
a good indicator of the performance of our algorithm on real-world
tasks. We aim to improve upon the simple, human-supervised approaches
to fault detection used in this field
\citep{wagner2006guidelines}. 

\subsection{Datasets}

For a quantitative assessment, we used two semi-synthetic datasets
where a typical fault has been injected into clean sensor data. We
then analyzed qualitative performance on two real data sets with actual
faults. All measurements are given in meters, with samples spaced in
increments of approximately 30 minutes.

%% We return to the water-level signal with the painting artifacts
%% plotted in Figure \ref{hgriver}.  To test the method described above,
%% we performed 10-step lookahead online prediction on the river-level
%% function using a zero-mean Gaussian process prior distribution and the
%% fault-bucket observation model.  The covariance function was chosen to
%% be a Mat\'{e}rn covariance with parameter $\nu = \nicefrac{5}{2}$
%% \citep{gpml}.  The hyperparameters of the non-fault model (the
%% characteristic input and output scales and the noise variance
%% $\sigma_n^2$) were learned offline via maximum-likelihood--II
%% estimation on a disjoint segment of uncorrupted data.  The fault
%% variance $\sigma_f^2$ was set to $0.2$ and the fault prior $\alpha$
%% was chosen to be $10\%$.

%% Figure \ref{faultpredictions} shows the results.  The predictions
%% follow the true signal much better than the model that generated the
%% results in Figure \ref{normalpredictions}.  Additionally, the model
%% detected the faulty observations with reasonable accuracy, despite the
%% very small amount of prior knowledge about faults that the
%% fault-bucket model incorporates beyond ``faulty observations are
%% unpredictable.''

The datasets are described below.

\subsubsection{Synthetic bias fault}
Our first synthetic example concerns a simple sensor error where
measurements are temporarily adjusted by a constant offset, but
otherwise remain accurate. This could happen if the sensor undergoes
physical trauma which results in a loss of calibration.

\subsubsection{Synthetic anomaly}
In this dataset, the water level rises quickly, but smoothly, before
returning back to normal. This would be indicative of a genuine
environmental event such as a flash flood.

\subsubsection{Painting}
Painting is an error that occurs when ice builds on a sensor obscuring
some of the readings. It is characterized by frequent sensor spikes
interlaced with the original, and still accurate signal.

\subsubsection{``Fishkiller''}
Our final dataset, which we dub ``fishkiller'', comes from a sensor
near a dam on a river in British Columbia, Canada. It contains an
otherwise normal water level-reading that is occasionally interrupted
by a short period of rapid oscillation. This occurs when dam operators
open and close the floodgates too quickly. When this happens, the
water level on the other side of the dam experiences a rapid drop
during which time salmon can become trapped on the shores and
stranded, leading to suffocation. Detecting these events is critical
to proper regulation of dams in order to save the lives these fish.

\subsection{Implementation}

We implemented the algorithm described in Section \ref{bucket} in
\textsc{matlab} to address the task of 1-step-lookahead time-series
prediction.  A sliding window of size 100 was used to predict the
value of the next observation.  Each dataset was recentered so that a
zero prior mean function was appropriate, and the functions were all
modeled using a Mat\'{e}rn covariance with parameter $\nu =
\nicefrac{5}{2}$ \citep{gpml}.  The hyperparameters for this
covariance, including the normal observation noise variance
$\sigma_n^2$ were learned using training data nearby but not included
in the test datasets.  The unknown fault noise variance $\sigma_f^2$
was marginalized using Bayesian Monte Carlo, with 7 samples in this
parameter.  The prior probability of an observation being faulty was
set to a constant value of 1\% throughout each test.

As a basis for comparison, we employed the widely used Kalman
filter. We first trained the parameters on a set of clean data (the
same as that used for the fault-bucket approach), and on this dataset
we attempted to make one-step-lookahead predictions in an online
fashion. We then take the mean-squared error and set a threshold at 3
times the standard deviation on the training set. Using this
threshold, when processing test data we look to see at each point if
the Kalman filter prediction deviated more than this amount from the
expected prediction. If it did, we label it a fault and discard the
point for future predictions.

\subsection{Results}
Figures \ref{compare} and \ref{justfb} shows the performance of the
fault-bucket prediction algorithm on the four datasets, as well as the
performance of the Kalman filter on the synthetic datasets for
comparison.  In all cases, the fault-bucket algorithm did an excellent
job of identifying faults when they occur, and made excellent
predictions in all cases.  The faults in the synthetic bias, painting,
and fishkiller datasets were identified almost perfectly, with a small
number of false negatives in the latter two.  Most of the fault in the
synthetic anomaly dataset was identified; however, the algorithm
required a number of readings at the beginning of the smooth fault
interval before it was able to conclude conclusively that the sharp
rise was not part of the normal signal.

Table \ref{tbl:results} displays quantitative measures of performance
for the fault-bucket and Kalman filter algorithm on the synthetic
datasets.  The performance is remarkably better under mean-squared
predictive error as well as the likelihood of the actual data.  The
detection rates for the faulty points is also excellent, with the
fault bucket algorithm identifying the fault in the synthetic anomaly
dataset somewhat slower than the Kalman filter.

\begin{table}
  \centering
  \caption{Quantitative comparison of different algorithms on the
    synthetic datasets.  For each dataset, the mean squared error
    (\textsc{mse}), the log likelihood of the true data ($\log p(\bm{y}
    \given \bm{x}, \cm{M})$), and the true-positive and false-positive rates
    of detection for faulty points (\textsc{tpr} and \textsc{fpr}),
    respectively, are shown.  The top half of the table refers to the
    bias dataset; the bottom half to the change-in-dynamics
    dataset. The better of each pair of results is highlighted in
    bold.}
  \label{tbl:results}
  \begin{tabular}{ccccc}
    \toprule
    Method & \scshape{mse} & $\log p(\bm{y} \given \bm{x}, \cm{M})$ & \scshape{tpr} & \scshape{fpr} \\
    \cmidrule{1-1} \cmidrule(l){2-5} 
    \scshape{kf} & 0.894 & $-7.11 \times 10^5$ & 0.053 & 0.073 \\
    \scshape{fb} & \textbf{0.034} & \textbf{-360} & \textbf{0.997} & \textbf{0.011} \\
    \midrule
    \scshape{kf} & 0.360 & $-3.54 \times 10^4$ & \textbf{0.835} & 0.046 \\
    \scshape{fb} & \textbf{0.075} & $\mathbf{-1.30 \times 10^4}$ & 0.798 & \textbf{0.006} \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Conclusions}
We have proposed a novel algorithm which we called the ``fault
bucket,'' for managing time-series data corrupted by faults unknown
ahead of time. The chief theoretical contribution of the paper is a
sequential algorithm for marginalizing over the possible faultiness of
all observations. This allows for fast, principled prediction in the
presence of unknown faults.

\bibliography{fault_bucket_icml_2011}
\bibliographystyle{icml2011}

\end{document}


